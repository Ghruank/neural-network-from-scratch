{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    train_images = train_images.reshape(train_images.shape[0], -1).astype('float32') / 255\n",
    "    test_images = test_images.reshape(test_images.shape[0], -1).astype('float32') / 255\n",
    "    \n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
    "    \n",
    "    return (train_images, train_labels), (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2./layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    expZ = np.exp(Z_shifted)\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    cache = {}\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        Z = np.dot(parameters['W' + str(l)], A) + parameters['b' + str(l)]\n",
    "        if l == L:\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "        cache['A' + str(l)] = A\n",
    "        cache['Z' + str(l)] = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    cross_entropy_cost = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "    L2_regularization_cost = (lambd / (2 * m)) * sum(np.sum(np.square(parameters['W' + str(l)])) for l in range(1, len(parameters) // 2 + 1))\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y, lambd):\n",
    "    grads = {}\n",
    "    L = len(parameters) // 2\n",
    "    m = X.shape[1]\n",
    "    AL = cache['A' + str(L)]\n",
    "    \n",
    "    # Gradient of the cost with respect to ZL\n",
    "    dZL = AL - Y\n",
    "    grads['dW' + str(L)] = (np.dot(dZL, cache['A' + str(L-1)].T) + lambd * parameters['W' + str(L)]) / m\n",
    "    grads['db' + str(L)] = np.sum(dZL, axis=1, keepdims=True) / m\n",
    "    grads['dZ' + str(L)] = dZL\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        dA = np.dot(parameters['W' + str(l+1)].T, grads['dZ' + str(l+1)])\n",
    "        dZ = dA * (cache['Z' + str(l)] > 0)\n",
    "        if l > 1:\n",
    "            grads['dW' + str(l)] = (np.dot(dZ, cache['A' + str(l-1)].T) + lambd * parameters['W' + str(l)]) / m\n",
    "        else:\n",
    "            grads['dW' + str(l)] = (np.dot(dZ, X.T) + lambd * parameters['W' + str(l)]) / m\n",
    "        grads['db' + str(l)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        grads['dZ' + str(l)] = dZ\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, lambd=0.7, mini_batch_size=64, seed=0):\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        minibatches = random_mini_batches(X.T, Y.T, mini_batch_size, seed)\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            minibatch_X, minibatch_Y = minibatch\n",
    "            AL, cache = forward_propagation(minibatch_X, parameters)\n",
    "            cost = compute_cost(AL, minibatch_Y, parameters, lambd)\n",
    "            grads = backward_propagation(parameters, cache, minibatch_X, minibatch_Y, lambd)\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [train_images.shape[1], 128,128,128,64, 64, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 10.291111847111548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parameters, costs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0075\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, lambd, mini_batch_size, seed)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m minibatches:\n\u001b[0;32m      9\u001b[0m     minibatch_X, minibatch_Y \u001b[38;5;241m=\u001b[39m minibatch\n\u001b[1;32m---> 10\u001b[0m     AL, cache \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     cost \u001b[38;5;241m=\u001b[39m compute_cost(AL, minibatch_Y, parameters, lambd)\n\u001b[0;32m     12\u001b[0m     grads \u001b[38;5;241m=\u001b[39m backward_propagation(parameters, cache, minibatch_X, minibatch_Y, lambd)\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m      4\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, L \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m L:\n\u001b[0;32m      9\u001b[0m         A \u001b[38;5;241m=\u001b[39m softmax(Z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters, costs = model(train_images, train_labels, layers_dims, learning_rate=0.0075, num_iterations=3000, lambd=0.7, mini_batch_size=64, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
